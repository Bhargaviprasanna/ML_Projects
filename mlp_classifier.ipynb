{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcXvmYsmZaU+ORYBP9rRgo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra3PhSv2_BIJ"
      },
      "source": [
        "###### Please once look at it and understand\n",
        "https://www.kaggle.com/granjithkumar/nlp-with-women-clothing-reviews\n",
        "\n",
        "help for data cleaning\n",
        "https://www.kaggle.com/suyashpratapsingh/eda-and-sentiment-analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfQxjrETw9gJ"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJmXBkjTxHts",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "d3a6453a-3fcf-4195-a2ba-89eef8f2b2da"
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/hanzhang0420/Women-Clothing-E-commerce/master/Womens%20Clothing%20E-Commerce%20Reviews.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Clothing ID</th>\n",
              "      <th>Age</th>\n",
              "      <th>Title</th>\n",
              "      <th>Review Text</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Recommended IND</th>\n",
              "      <th>Positive Feedback Count</th>\n",
              "      <th>Division Name</th>\n",
              "      <th>Department Name</th>\n",
              "      <th>Class Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>767</td>\n",
              "      <td>33</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Initmates</td>\n",
              "      <td>Intimate</td>\n",
              "      <td>Intimates</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1080</td>\n",
              "      <td>34</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>General</td>\n",
              "      <td>Dresses</td>\n",
              "      <td>Dresses</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1077</td>\n",
              "      <td>60</td>\n",
              "      <td>Some major design flaws</td>\n",
              "      <td>I had such high hopes for this dress and reall...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>General</td>\n",
              "      <td>Dresses</td>\n",
              "      <td>Dresses</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1049</td>\n",
              "      <td>50</td>\n",
              "      <td>My favorite buy!</td>\n",
              "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>General Petite</td>\n",
              "      <td>Bottoms</td>\n",
              "      <td>Pants</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>847</td>\n",
              "      <td>47</td>\n",
              "      <td>Flattering shirt</td>\n",
              "      <td>This shirt is very flattering to all due to th...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>General</td>\n",
              "      <td>Tops</td>\n",
              "      <td>Blouses</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Clothing ID  Age  ...   Division Name Department Name  Class Name\n",
              "0           0          767   33  ...       Initmates        Intimate   Intimates\n",
              "1           1         1080   34  ...         General         Dresses     Dresses\n",
              "2           2         1077   60  ...         General         Dresses     Dresses\n",
              "3           3         1049   50  ...  General Petite         Bottoms       Pants\n",
              "4           4          847   47  ...         General            Tops     Blouses\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYhwMVctxH1q",
        "outputId": "88b626f4-ce5d-4bf9-dbda-8baf964bb2c3"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 23486 entries, 0 to 23485\n",
            "Data columns (total 11 columns):\n",
            " #   Column                   Non-Null Count  Dtype \n",
            "---  ------                   --------------  ----- \n",
            " 0   Unnamed: 0               23486 non-null  int64 \n",
            " 1   Clothing ID              23486 non-null  int64 \n",
            " 2   Age                      23486 non-null  int64 \n",
            " 3   Title                    19676 non-null  object\n",
            " 4   Review Text              22641 non-null  object\n",
            " 5   Rating                   23486 non-null  int64 \n",
            " 6   Recommended IND          23486 non-null  int64 \n",
            " 7   Positive Feedback Count  23486 non-null  int64 \n",
            " 8   Division Name            23472 non-null  object\n",
            " 9   Department Name          23472 non-null  object\n",
            " 10  Class Name               23472 non-null  object\n",
            "dtypes: int64(6), object(5)\n",
            "memory usage: 2.0+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFNof4ei-Uwp"
      },
      "source": [
        "df = df.dropna(axis=0, subset=['Review Text']) #null review rows are dropped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEJV-veg-cap",
        "outputId": "b68fc678-94a3-4836-f88d-9d996c5b016a"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 22641 entries, 0 to 23485\n",
            "Data columns (total 11 columns):\n",
            " #   Column                   Non-Null Count  Dtype \n",
            "---  ------                   --------------  ----- \n",
            " 0   Unnamed: 0               22641 non-null  int64 \n",
            " 1   Clothing ID              22641 non-null  int64 \n",
            " 2   Age                      22641 non-null  int64 \n",
            " 3   Title                    19675 non-null  object\n",
            " 4   Review Text              22641 non-null  object\n",
            " 5   Rating                   22641 non-null  int64 \n",
            " 6   Recommended IND          22641 non-null  int64 \n",
            " 7   Positive Feedback Count  22641 non-null  int64 \n",
            " 8   Division Name            22628 non-null  object\n",
            " 9   Department Name          22628 non-null  object\n",
            " 10  Class Name               22628 non-null  object\n",
            "dtypes: int64(6), object(5)\n",
            "memory usage: 2.1+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WmVdunsyhg6"
      },
      "source": [
        "### Clean text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV-d4MAsWB9g"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "def tokens(words):\n",
        "    words = re.sub(\"[^a-zA-Z]\",\" \", words)\n",
        "    text = words.lower().split()                   \n",
        "    return \" \".join(text)\n",
        "df['Review_clear'] = df['Review Text'].apply(tokens)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAwY03UklIY5"
      },
      "source": [
        "df['Review_clear'] = df['Review_clear'].astype(str)\n",
        "stop_words = stopwords.words('english')\n",
        "#clothing stopwords\n",
        "clothes =['dress','color','wear','top','sweater','material','shirt','jeans','pant',\n",
        "          'skirt','order','white','black','fabric','blouse','sleeve','even', 'jacket']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok0M6A2JlSAA"
      },
      "source": [
        "def stopwords(review):\n",
        "    text = [word.lower() for word in review.split() if word.lower() not in stop_words and word.lower() not in clothes]\n",
        "    return \" \".join(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMT95DhvlUp4"
      },
      "source": [
        "df['Review_clear'] = df['Review_clear'].apply(stopwords)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ppm06RIliy_",
        "outputId": "dfcbff7e-1a12-40c8-d50d-9e198cd758c7"
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "def lemma(text):\n",
        "    lem_text = [lem.lemmatize(word) for word in text.split()]\n",
        "    return \" \".join(lem_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr-a2YmqltA4"
      },
      "source": [
        "df['Review_clear'] = df['Review_clear'].apply(lemma)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIMsMe5-xIFi"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = df['Review_clear']\n",
        "ylabels = df['Recommended IND']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10FnAQ5n9Fl-"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "# from sklearn.ensemble import StackingClassifier \n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier3 = MLPClassifier(hidden_layer_sizes=(32,64), learning_rate=\"adaptive\", max_iter=30,verbose=10, activation='relu', random_state=1, solver='adam')\n",
        "classifier2 =  LinearSVC(C=0.01, dual=False, loss='squared_hinge', penalty='l2', tol=0.0001, random_state=42)\n",
        "classifier1 = LogisticRegression()\n",
        "models = [classifier1, classifier2, classifier3]\n",
        "\n",
        "f_model = LogisticRegression()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dzF1al0BUgj"
      },
      "source": [
        "from mlxtend.classifier import StackingClassifier\n",
        "classifier = StackingClassifier(classifiers = models, meta_classifier = f_model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRSXdSP38-mu"
      },
      "source": [
        "vect = TfidfVectorizer(ngram_range=(1,2))\n",
        "vect.fit(X_train, y_train)\n",
        "# (vect.idf_).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx5P8Fqm96qt"
      },
      "source": [
        "transf = TfidfTransformer()\n",
        "transf.fit_transform(X_train,y_train)\n",
        "(transf.id_).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhu7h2et9sd-",
        "outputId": "f996048a-bd84-41a2-e279-c5a1ce9976e1"
      },
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.svm import LinearSVC\n",
        "# from sklearn.naive_bayes import MultinomialNB\n",
        "# classifier = MultinomialNB()\n",
        "# classifier = LogisticRegression()\n",
        "# classifier = SVC(kernel='linear')\n",
        "# classifier =  LinearSVC(C=0.01, dual=False, loss='squared_hinge', penalty='l2', tol=0.0001)\n",
        "# Create pipeline using Bag of Words\n",
        "pipe = Pipeline([(\"vectorizer\", TfidfVectorizer(ngram_range=(1,2))),\n",
        "                 ('transformer', TfidfTransformer()),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# model generation\n",
        "pipe.fit(X_train,y_train)\n",
        "# The combination of penalty='l2' and loss='hinge' are not supported when dual=False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.50189010\n",
            "Iteration 2, loss = 0.26201187\n",
            "Iteration 3, loss = 0.05033275\n",
            "Iteration 4, loss = 0.00649542\n",
            "Iteration 5, loss = 0.00292691\n",
            "Iteration 6, loss = 0.00194116\n",
            "Iteration 7, loss = 0.00150469\n",
            "Iteration 8, loss = 0.00126880\n",
            "Iteration 9, loss = 0.00112162\n",
            "Iteration 10, loss = 0.00102072\n",
            "Iteration 11, loss = 0.00094728\n",
            "Iteration 12, loss = 0.00089081\n",
            "Iteration 13, loss = 0.00084572\n",
            "Iteration 14, loss = 0.00080821\n",
            "Iteration 15, loss = 0.00077624\n",
            "Iteration 16, loss = 0.00074841\n",
            "Iteration 17, loss = 0.00072356\n",
            "Iteration 18, loss = 0.00070103\n",
            "Iteration 19, loss = 0.00068031\n",
            "Iteration 20, loss = 0.00066102\n",
            "Iteration 21, loss = 0.00064291\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 2), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_patt...\n",
              "                                    meta_classifier=LogisticRegression(C=1.0,\n",
              "                                                                       class_weight=None,\n",
              "                                                                       dual=False,\n",
              "                                                                       fit_intercept=True,\n",
              "                                                                       intercept_scaling=1,\n",
              "                                                                       l1_ratio=None,\n",
              "                                                                       max_iter=100,\n",
              "                                                                       multi_class='auto',\n",
              "                                                                       n_jobs=None,\n",
              "                                                                       penalty='l2',\n",
              "                                                                       random_state=None,\n",
              "                                                                       solver='lbfgs',\n",
              "                                                                       tol=0.0001,\n",
              "                                                                       verbose=0,\n",
              "                                                                       warm_start=False),\n",
              "                                    store_train_meta_features=False,\n",
              "                                    use_clones=True,\n",
              "                                    use_features_in_secondary=False,\n",
              "                                    use_probas=False, verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDwcnTzrz0ag"
      },
      "source": [
        "\n",
        "# pipe['transformer'] shape\n",
        "print(pipe['transformer'].idf_)\n",
        "(pipe['transformer'].idf_).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbvE4o879eEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b63885b2-a9f1-4fec-ed4d-9a1371124a1e"
      },
      "source": [
        "#Predicting class\n",
        "predicted = pipe.predict(X_test)\n",
        "predicted"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, ..., 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goQCIEfj_WpB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eea791f-cc5b-406a-cb5a-2ce710c2f213"
      },
      "source": [
        "#using mlp alone\n",
        "#Confusion matrix \n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted)\n",
        "print(cm)\n",
        "score = f1_score(y_test, predicted)\n",
        "print(score*100)\n",
        "print(\"Accuracy:\",accuracy_score(y_test, predicted)*100)\n",
        "print('Classification report is ')\n",
        "print(classification_report(y_test, predicted))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 459  362]\n",
            " [ 126 3582]]\n",
            "93.62258233141662\n",
            "Accuracy: 89.22499448001766\n",
            "Classification report is \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.56      0.65       821\n",
            "           1       0.91      0.97      0.94      3708\n",
            "\n",
            "    accuracy                           0.89      4529\n",
            "   macro avg       0.85      0.76      0.79      4529\n",
            "weighted avg       0.89      0.89      0.88      4529\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Dyf1__CD2tL"
      },
      "source": [
        "#stcked with 3\n",
        "[[ 459  362]\n",
        " [ 126 3582]]\n",
        "93.62258233141662\n",
        "Accuracy: 89.22499448001766\n",
        "Classification report is \n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.78      0.56      0.65       821\n",
        "           1       0.91      0.97      0.94      3708\n",
        "\n",
        "    accuracy                           0.89      4529\n",
        "   macro avg       0.85      0.76      0.79      4529\n",
        "weighted avg       0.89      0.89      0.88      4529\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6h2IN8JDxHu"
      },
      "source": [
        "# Stcked classi with 2 classifiers\n",
        "[[ 440  401]\n",
        " [ 126 3562]]\n",
        "93.11201150176448\n",
        "Accuracy: 88.36387723559285\n",
        "Classification report is \n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.78      0.52      0.63       841\n",
        "           1       0.90      0.97      0.93      3688\n",
        "\n",
        "    accuracy                           0.88      4529\n",
        "   macro avg       0.84      0.74      0.78      4529\n",
        "weighted avg       0.88      0.88      0.87      4529\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czzrqe_zvA2I"
      },
      "source": [
        "# mlp alone\n",
        "[[ 456  372]\n",
        " [ 135 3566]]\n",
        "93.36300562900904\n",
        "Accuracy: 88.80547582247736\n",
        "Classification report is \n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.77      0.55      0.64       828\n",
        "           1       0.91      0.96      0.93      3701\n",
        "\n",
        "    accuracy                           0.89      4529\n",
        "   macro avg       0.84      0.76      0.79      4529\n",
        "weighted avg       0.88      0.89      0.88      4529\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6pHXf1tFs5C"
      },
      "source": [
        "## Diff Classifiers\n",
        "1. Logistic Regression\n",
        ">> [[ 485  378]\n",
        ">>[ 189 3477]]\n",
        ">> f1_score: 92.46110889509374\n",
        "\n",
        "2. SVM Classifier\n",
        ">> [[ 514  349]\n",
        ">> [ 250 3416]]\n",
        ">> f1_score: 91.93917373166465\n",
        "\n",
        "3. MultinomialNB\n",
        ">> [[ 132  700]\n",
        ">>  [ 632 3065]]\n",
        ">> 82.14955775931386\n",
        ">> Accuracy: 70.58953411349084"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8jKjcRTHvwJ"
      },
      "source": [
        "## pair of words\n",
        "n_gram = (1,2)\n",
        "\n",
        "1. MultinomialNB\n",
        ">>[[ 137  692][  16 3684]]\n",
        "  91.2332838038633\n",
        "   Accuracy: 84.36741002428792\n",
        "2. SVM Classifier\n",
        ">> [[ 521  308]\n",
        " [ 210 3490]]\n",
        " 93.0914910642838\n",
        " Accuracy: 88.56259659969088\n",
        "\n",
        "3. Logistic Regression\n",
        ">> [[ 512  317]\n",
        "   [ 157 3543]]\n",
        "   93.73015873015873\n",
        "   Accuracy: 89.53411349083683"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQWN4Pnflf7a"
      },
      "source": [
        "pip install tpot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9Y_WX5Bk1Xb"
      },
      "source": [
        "from tpot import TPOTClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Instantiate TPOTClassifier\n",
        "classifier = TPOTClassifier(\n",
        "    generations=5,\n",
        "    population_size=20,\n",
        "    verbosity=2,\n",
        "    scoring='roc_auc',\n",
        "    random_state=42,\n",
        "    disable_update_check=True,\n",
        "    config_dict='TPOT sparse'\n",
        ")\n",
        "# tpot.fit(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "899JaxvHxIK6"
      },
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn.naive_bayes import MultinomialNB\n",
        "# classifier = MultinomialNB()\n",
        "# classifier = LogisticRegression()\n",
        "# classifier = SVC(kernel='linear')\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "pipe = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', bow_vector),\n",
        "                  ('classifier', classifier) ])\n",
        "\n",
        "# model generation\n",
        "pipe.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBq03IoW4kqo"
      },
      "source": [
        "print('\\nBest pipeline steps:', end='\\n')\n",
        "for idx, (name, transform) in enumerate(pipe.fitted_pipeline_.steps, start=1):\n",
        "    # Print idx and transform\n",
        "    print(f'{idx}. {transform}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE5rJ5ZQ8_p2"
      },
      "source": [
        "# Best pipeline found out using TPOT\n",
        "## Best pipeline: LinearSVC(input_matrix, C=0.01, dual=False, loss=squared_hinge, penalty=l2, tol=0.0001)\n",
        "Pipeline(memory=None,\n",
        "         steps=[('cleaner', <__main__.predictors object at 0x7fc97bd0bf90>),\n",
        "                ('vectorizer',\n",
        "                 CountVectorizer(analyzer='word', binary=False,\n",
        "                                 decode_error='strict',\n",
        "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
        "                                 input='content', lowercase=True, max_df=1.0,\n",
        "                                 max_features=None, min_df=1,\n",
        "                                 ngram_range=(1, 2), preprocessor=None,\n",
        "                                 stop_words=None, strip_accents=None,\n",
        "                                 t...\n",
        "                 TPOTClassifier(config_dict='TPOT sparse', crossover_rate=0.1,\n",
        "                                cv=5, disable_update_check=True,\n",
        "                                early_stop=None, generations=5, log_file=None,\n",
        "                                max_eval_time_mins=5, max_time_mins=None,\n",
        "                                memory=None, mutation_rate=0.9, n_jobs=1,\n",
        "                                offspring_size=None,\n",
        "                                periodic_checkpoint_folder=None,\n",
        "                                population_size=20, random_state=42,\n",
        "                                scoring='roc_auc', subsample=1.0, template=None,\n",
        "                                use_dask=False, verbosity=2,\n",
        "                                warm_start=False))],\n",
        "         verbose=False)"
      ]
    }
  ]
}